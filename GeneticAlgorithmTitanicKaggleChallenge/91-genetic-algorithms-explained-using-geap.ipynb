{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A quick note about myself, I'm Justin GÃ¼se, Data & Cloud enthusiast, reachable here or via Linkedin https://www.linkedin.com/in/justin-guese/. If you spot any errors or improvements feel free to comment!\n",
    "\n",
    "I can only recommend to everyone to use the Titanic Challenge as a way to dive into Machine Learning. Before diving into a Genetic Algorithm I would recommend the notebooks of \"LD Freeman\" and \"Gunes Evitan\", dealing with the \"basic\" approach of Data Science using Feature Engineering and the models as we know it:\n",
    "- https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "- https://www.kaggle.com/gunesevitan/advanced-feature-engineering-tutorial-with-titanic\n",
    "\n",
    "Now if you do not know yet the 1.0 scores in the Leaderboard are cheated, which \"Tarun\" demonstrated in his notebook:\n",
    "- https://www.kaggle.com/tarunpaparaju/how-top-lb-got-their-score-use-titanic-to-learn\n",
    "\n",
    "Your ML model should never reach a score of 1.0, because that basically means you are massively overfitting or including the \"solution\" in your dataset.\n",
    "\n",
    "Genetic Algorithms are a totally different approach. Imagine classic ML models as an \"art\", whilst Genetic Algorithms basically randomly try Millions of variables to improve the overall score, with several rounds where only the fittest variables will survive (hehe, Evolution jokes).\n",
    "My former Genetic model has been based on \"Akshat Goel\", check out his notebook here:\n",
    "- https://www.kaggle.com/akshat113/titanic-dataset-analysis-level-2\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. What is a Genetic Algorithm?\n",
    "2. My Algorithm to reach a score of 91% on the Titanic Dataset\n",
    "3. Build your own algorithm\n",
    "4. Building the basic score model using deap (library)\n",
    "5. Fine-Tuning of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Genetic Algorithm?\n",
    "As described above a Genetic Algorithm is not \"intelligent\" in the classic sense. It just tries Millions of random variables to achieve the best score. You can compare it to the evolution in that sense, that the algorithm tries out random \"new mutations\" in the formula, and chooses the winner each round to again mutate it in the next iteration. \n",
    "\n",
    "Some good introduction material about Genetic Algorithms:\n",
    "- https://www.youtube.com/watch?v=9c1qo1eU1kY\n",
    "- https://medium.com/analytics-vidhya/understanding-genetic-algorithms-in-the-artificial-intelligence-spectrum-7021b7cc25e7\n",
    "\n",
    "There is a python library for programming Genetic Algorithms which we will use later on:\n",
    "- https://github.com/DEAP/deap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  My Algorithm to reach a score of 91% on the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy import simplify, cos, sin, Symbol, Function, tanh, pprint, init_printing, exp\n",
    "from sympy.functions import Min,Max\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# the winner variables with former values after the hashtag\n",
    "A = 0.058823499828577    \n",
    "B = 0.841127 # 0.885868\n",
    "C = 0.138462007045746 \n",
    "D = 0.31830988618379069\n",
    "E = 2.810815 # 2.675679922103882 \n",
    "F = 0.63661977236758138\n",
    "G = 5.428569793701172   \n",
    "H = 3.1415926535897931\n",
    "I = 0.592158 #0.623655974864960\n",
    "J = 4.869778 #  2.770736 # 2.212120056152344\n",
    "K = 0.063467 # 1.5707963267948966\n",
    "L = -0.091481 # 0.094339601695538 \n",
    "M = 0.0821533 \n",
    "N = 0.720430016517639\n",
    "O = 0.230145 \n",
    "P = 9.89287 \n",
    "Q = 785 \n",
    "R = 1.07241 \n",
    "S = 281\n",
    "T = 734\n",
    "U = 5.3\n",
    "V = 67.0\n",
    "W = 2.484848\n",
    "X = 8.48635 \n",
    "Y = 63\n",
    "Z = 12.6275 \n",
    "AA = 0.735354 # 0.7\n",
    "AB = 727\n",
    "AC = 2.5\n",
    "AD = 2.6 \n",
    "AE = 0.3\n",
    "AF = 3.0\n",
    "AG = 0.226263 #0.1\n",
    "AH = 2.0\n",
    "AI = 12.4148\n",
    "AJ = 96\n",
    "AK = 0.130303 # 0.2\n",
    "AL = 176\n",
    "AM = 3.2\n",
    "BIG = [A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now may I present: The winning gen function, Inspired by Akshat's notebook:\n",
    "# https://www.kaggle.com/akshat113/titanic-dataset-analysis-level-2\n",
    "def GeneticFunction(data,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM):\n",
    "    return ((np.minimum( ((((A + data[\"Sex\"]) - np.cos((data[\"Pclass\"] / AH))) * AH)),  ((B))) * AH) +\n",
    "            np.maximum( ((data[\"SibSp\"] - AC)),  ( -(np.minimum( (data[\"Sex\"]),  (np.sin(data[\"Parch\"]))) * data[\"Pclass\"]))) +\n",
    "            (AG * ((np.minimum( (data[\"Sex\"]),  (((data[\"Parch\"] / AH) / AH))) * data[\"Age\"]) - data[\"Cabin\"])) +\n",
    "            np.minimum( ((np.sin((data[\"Parch\"] * ((data[\"Fare\"] - AA) * AH))) * AH)),  ((data[\"SibSp\"] / AH))) +\n",
    "            np.maximum( (np.minimum( ( -np.cos(data[\"Embarked\"])),  (C))),  (np.sin(((data[\"Cabin\"] - data[\"Fare\"]) * AH)))) +\n",
    "            -np.minimum( ((((data[\"Age\"] * data[\"Parch\"]) * data[\"Embarked\"]) + data[\"Parch\"])),  (np.sin(data[\"Pclass\"]))) +\n",
    "            np.minimum( (data[\"Sex\"]),  ((np.sin( -(data[\"Fare\"] * np.cos((data[\"Fare\"] * W)))) / AH))) +\n",
    "            np.minimum( ((O)),  (np.sin(np.minimum( (((V / AH) * np.sin(data[\"Fare\"]))),  (D))))) +\n",
    "            np.sin((np.sin(data[\"Cabin\"]) * (np.sin((Z)) * np.maximum( (data[\"Age\"]),  (data[\"Fare\"]))))) +\n",
    "            np.sin(((np.minimum( (data[\"Fare\"]),  ((data[\"Cabin\"] * data[\"Embarked\"]))) / AH) *  -data[\"Fare\"])) +\n",
    "            np.minimum( (((AD * data[\"SibSp\"]) * np.sin(((AJ) * np.sin(data[\"Cabin\"]))))),  (data[\"Parch\"])) +\n",
    "            np.sin(np.sin((np.maximum( (np.minimum( (data[\"Age\"]),  (data[\"Cabin\"]))),  ((data[\"Fare\"] * AK))) * data[\"Cabin\"]))) +\n",
    "            np.maximum( (np.sin(((AI) * (data[\"Age\"] / AH)))),  (np.sin((-AF * data[\"Cabin\"])))) +\n",
    "            (np.minimum( (np.sin((((np.sin(((data[\"Fare\"] * AH) * AH)) * AH) * AH) * AH))),  (data[\"SibSp\"])) / AH) +\n",
    "            ((data[\"Sex\"] - data[\"SibSp\"]) * (np.cos(((data[\"Embarked\"] - AA) + data[\"Age\"])) / AH)) +\n",
    "            ((np.sin(data[\"Cabin\"]) / AH) - (np.cos(np.minimum( (data[\"Age\"]),  (data[\"Embarked\"]))) * np.sin(data[\"Embarked\"]))) +\n",
    "            np.minimum( (AE),  ((data[\"Sex\"] * (J * (N - np.sin((data[\"Age\"] * AH))))))) +\n",
    "            (np.minimum( (np.cos(data[\"Fare\"])),  (np.maximum( (np.sin(data[\"Age\"])),  (data[\"Parch\"])))) * np.cos((data[\"Fare\"] / AH))) +\n",
    "            np.sin((data[\"Parch\"] * np.minimum( ((data[\"Age\"] - K)),  ((np.cos((data[\"Pclass\"] * AH)) / AH))))) +\n",
    "            (data[\"Parch\"] * (np.sin(((data[\"Fare\"] * (I * data[\"Age\"])) * AH)) / AH)) +\n",
    "            (D * np.cos(np.maximum( ((0.5 * data[\"Fare\"])),  ((np.sin(N) * data[\"Age\"]))))) +\n",
    "            (np.minimum( ((data[\"SibSp\"] / AH)),  (np.sin(((data[\"Pclass\"] - data[\"Fare\"]) * data[\"SibSp\"])))) * data[\"SibSp\"]) +\n",
    "            np.tanh((data[\"Sex\"] * np.sin((U * np.sin((data[\"Cabin\"] * np.cos(data[\"Fare\"]))))))) +\n",
    "            (np.minimum( (data[\"Parch\"]),  (data[\"Sex\"])) * np.cos(np.maximum( ((np.cos(data[\"Parch\"]) + data[\"Age\"])),  (AM)))) +\n",
    "            (np.minimum( (np.tanh(((data[\"Cabin\"] / AH) + data[\"Parch\"]))),  ((data[\"Sex\"] + np.cos(data[\"Age\"])))) / AH) +\n",
    "            (np.sin((np.sin(data[\"Sex\"]) * (np.sin((data[\"Age\"] * data[\"Pclass\"])) * data[\"Pclass\"]))) / AH) +\n",
    "            (data[\"Sex\"] * (np.cos(((data[\"Sex\"] + data[\"Fare\"]) * ((X) * (Y)))) / AH)) +\n",
    "            np.minimum( (data[\"Sex\"]),  ((np.cos((data[\"Age\"] * np.tanh(np.sin(np.cos(data[\"Fare\"]))))) / AH))) +\n",
    "            (np.tanh(np.tanh( -np.cos((np.maximum( (np.cos(data[\"Fare\"])),  (L)) * data[\"Age\"])))) / AH) +\n",
    "            (np.tanh(np.cos((np.cos(data[\"Age\"]) + (data[\"Age\"] + np.minimum( (data[\"Fare\"]),  (data[\"Age\"])))))) / AH) +\n",
    "            (np.tanh(np.cos((data[\"Age\"] * ((-AH + np.sin(data[\"SibSp\"])) + data[\"Fare\"])))) / AH) +\n",
    "            (np.minimum( (((S) - data[\"Fare\"])),  (np.sin((np.maximum( ((AL)),  (data[\"Fare\"])) * data[\"SibSp\"])))) * AH) +\n",
    "            np.sin(((np.maximum( (data[\"Embarked\"]),  (data[\"Age\"])) * AH) * (((Q) * H) * data[\"Age\"]))) +\n",
    "            np.minimum( (data[\"Sex\"]),  (np.sin( -(np.minimum( ((data[\"Cabin\"] / AH)),  (data[\"SibSp\"])) * (data[\"Fare\"] / AH))))) +\n",
    "            np.sin(np.sin((data[\"Cabin\"] * (data[\"Embarked\"] + (np.tanh( -data[\"Age\"]) + data[\"Fare\"]))))) +\n",
    "            (np.cos(np.cos(data[\"Fare\"])) * (np.sin((data[\"Embarked\"] - ((T) * data[\"Fare\"]))) / AH)) +\n",
    "            ((np.minimum( (data[\"SibSp\"]),  (np.cos(data[\"Fare\"]))) * np.cos(data[\"SibSp\"])) * np.sin((data[\"Age\"] / AH))) +\n",
    "            (np.sin((np.sin((data[\"SibSp\"] * np.cos((data[\"Fare\"] * AH)))) + (data[\"Cabin\"] * AH))) / AH) +\n",
    "            (((data[\"Sex\"] * data[\"SibSp\"]) * np.sin(np.sin( -(data[\"Fare\"] * data[\"Cabin\"])))) * AH) +\n",
    "            (np.sin((data[\"SibSp\"] * ((((G + V) * AH) / AH) * data[\"Age\"]))) / AH) +\n",
    "            (data[\"Pclass\"] * (np.sin(((data[\"Embarked\"] * data[\"Cabin\"]) * (data[\"Age\"] - (R)))) / AH)) +\n",
    "            (np.cos((((( -data[\"SibSp\"] + data[\"Age\"]) + data[\"Parch\"]) * data[\"Embarked\"]) / AH)) / AH) +\n",
    "            (D * np.sin(((data[\"Age\"] * ((data[\"Embarked\"] * np.sin(data[\"Fare\"])) * AH)) * AH))) +\n",
    "            ((np.minimum( ((data[\"Age\"] * A)),  (data[\"Sex\"])) - F) * np.tanh(np.sin(data[\"Pclass\"]))) +\n",
    "            -np.minimum( ((np.cos(((AB) * ((data[\"Fare\"] + data[\"Parch\"]) * AH))) / AH)),  (data[\"Fare\"])) +\n",
    "            (np.minimum( (np.cos(data[\"Fare\"])),  (data[\"SibSp\"])) * np.minimum( (np.sin(data[\"Parch\"])),  (np.cos((data[\"Embarked\"] * AH))))) +\n",
    "            (np.minimum( (((data[\"Fare\"] / AH) - E)),  (C)) * np.sin((K * data[\"Age\"]))) +\n",
    "            np.minimum( ((M)),  (((np.sin(data[\"Fare\"]) + data[\"Embarked\"]) - np.cos((data[\"Age\"] * (P)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helper Functions to clean Titanic data and replace categorical values with numbers. \n",
    "# I can recommend the \"Advanced Feature Engineering\" notebook mentioned above for a deep dive\n",
    "# into why and how this is done\n",
    "\n",
    "def CleanData(data):\n",
    "    # Sex\n",
    "    data.drop(['Ticket', 'Name'], inplace=True, axis=1)\n",
    "    data.Sex.fillna('0', inplace=True)\n",
    "    data.loc[data.Sex != 'male', 'Sex'] = 0\n",
    "    data.loc[data.Sex == 'male', 'Sex'] = 1\n",
    "    # Cabin\n",
    "    data.Cabin.fillna('0', inplace=True)\n",
    "    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n",
    "    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2\n",
    "    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3\n",
    "    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4\n",
    "    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5\n",
    "    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6\n",
    "    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7\n",
    "    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8\n",
    "    # Embarked\n",
    "    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n",
    "    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n",
    "    data.loc[data.Embarked == 'S', 'Embarked'] = 3\n",
    "    data.Embarked.fillna(0, inplace=True)\n",
    "    data.fillna(-1, inplace=True)\n",
    "    return data.astype(float)\n",
    "\n",
    "# This function rounds values to either 1 or 0, because the GeneticFunction below returns floats and no\n",
    "# definite values\n",
    "def Outputs(data):\n",
    "    return np.round(1.-(1./(1.+np.exp(-data))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data\n",
    "raw_train = pd.read_csv('../input/titanic/train.csv')\n",
    "raw_test = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "cleanedTrain = CleanData(raw_train)\n",
    "cleanedTest = CleanData(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score:  92.70482603815937\n"
     ]
    }
   ],
   "source": [
    "# run a check on the Training dataset. See section \"Programm your own gen. algorithm\" below on how to \n",
    "# construct your own genetic algorithm\n",
    "thisArray = BIG.copy()\n",
    "testPredictions = Outputs(GeneticFunction(cleanedTrain,thisArray[0],thisArray[1],thisArray[2],thisArray[3],thisArray[4],thisArray[5],thisArray[6],thisArray[7],thisArray[8],thisArray[9],thisArray[10],thisArray[11],thisArray[12],thisArray[13],thisArray[14],thisArray[15],thisArray[16],thisArray[17],thisArray[18],thisArray[19],thisArray[20],thisArray[21],thisArray[22],thisArray[23],thisArray[24],thisArray[25],thisArray[26],thisArray[27],thisArray[28],thisArray[29],thisArray[30],thisArray[31],thisArray[32],thisArray[33],thisArray[34],thisArray[35],thisArray[36],thisArray[37],thisArray[38]))\n",
    "pdcheck = pd.DataFrame({'Survived': testPredictions.astype(int)})\n",
    "ret = pdcheck.Survived.where(pdcheck[\"Survived\"].values==cleanedTrain[\"Survived\"].values).notna()\n",
    "t,f = ret.value_counts()\n",
    "score = 100/(t+f)*t\n",
    "print(\"Training set score: \",score)\n",
    "# remember this is the score on our training set which is not the same as having the score on the test set\n",
    "# which is the result we see in Kaggle (almost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict results using Genetic Function on our Test data\n",
    "testPredictions = Outputs(GeneticFunction(cleanedTest,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM))\n",
    "pdtest = pd.DataFrame({'PassengerId': cleanedTest.PassengerId.astype(int),\n",
    "                        'Survived': testPredictions.astype(int)})\n",
    "pdtest.to_csv('submission.csv', index=False)\n",
    "pdtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build your own algorithm\n",
    "Now let us have a look on how you are able to build your own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler,MinMaxScaler\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now this number indicates the number of generations, which can quite long.\n",
    "# I recommend something around the number of 1000, but 100 is timewise okay (~5 min)\n",
    "# Currently it is at 10 to reduce runtime. Of course the more iterations the better the algorithm\n",
    "# Some code taken from: https://github.com/innjoshka/Genetic-Programming-Titanic-Kaggle\n",
    "HOWMANYITERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_deap(evolved_train):\n",
    "    global HOWMANYITERS\n",
    "    import operator\n",
    "    import math\n",
    "    import random\n",
    "\n",
    "\n",
    "    from deap import algorithms\n",
    "    from deap import base, creator\n",
    "    from deap import tools\n",
    "    from deap import gp\n",
    "\n",
    "    # dropping Survived and Passenger ID because we can not use them for training\n",
    "    outputs = evolved_train['Survived'].values.tolist()\n",
    "    evolved_train = evolved_train.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
    "    inputs = evolved_train.values.tolist() # to np array\n",
    "    \n",
    "\n",
    "\n",
    "    def protectedDiv(left, right):\n",
    "        try:\n",
    "            return left / right\n",
    "        except ZeroDivisionError:\n",
    "            return 1\n",
    "\n",
    "    def randomString(stringLength=10):\n",
    "        \"\"\"Generate a random string of fixed length \"\"\"\n",
    "        letters = string.ascii_lowercase\n",
    "        return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "    #choosing Primitives\n",
    "    pset = gp.PrimitiveSet(\"MAIN\", len(evolved_train.columns))  # add here\n",
    "    pset.addPrimitive(operator.add, 2)\n",
    "    pset.addPrimitive(operator.sub, 2)\n",
    "    pset.addPrimitive(operator.mul, 2)\n",
    "    pset.addPrimitive(protectedDiv, 2)\n",
    "    pset.addPrimitive(math.cos, 1)\n",
    "    pset.addPrimitive(math.sin, 1)\n",
    "    pset.addPrimitive(math.tanh,1)\n",
    "    pset.addPrimitive(max, 2)\n",
    "    pset.addPrimitive(min, 2)\n",
    "    pset.addEphemeralConstant(randomString(), lambda: random.uniform(-10,10))\n",
    "    # 50 as a precaution. 34 would be enough\n",
    "    pset.renameArguments(ARG0='x1')\n",
    "    pset.renameArguments(ARG1='x2')\n",
    "    pset.renameArguments(ARG2='x3')\n",
    "    pset.renameArguments(ARG3='x4')\n",
    "    pset.renameArguments(ARG4='x5')\n",
    "    pset.renameArguments(ARG5='x6')\n",
    "    pset.renameArguments(ARG6='x7')\n",
    "    pset.renameArguments(ARG7='x8')\n",
    "    pset.renameArguments(ARG8='x9')\n",
    "    pset.renameArguments(ARG9='x10')\n",
    "    pset.renameArguments(ARG10='x11')\n",
    "    pset.renameArguments(ARG11='x12')\n",
    "    pset.renameArguments(ARG12='x13')\n",
    "    pset.renameArguments(ARG13='x14')\n",
    "    pset.renameArguments(ARG14='x15')\n",
    "    pset.renameArguments(ARG15='x16')\n",
    "    pset.renameArguments(ARG16='x17')\n",
    "    pset.renameArguments(ARG17='x18')\n",
    "    pset.renameArguments(ARG18='x19')\n",
    "    pset.renameArguments(ARG19='x20')\n",
    "    pset.renameArguments(ARG20='x21')\n",
    "    pset.renameArguments(ARG21='x22')\n",
    "    pset.renameArguments(ARG22='x23')\n",
    "    pset.renameArguments(ARG23='x24')\n",
    "    pset.renameArguments(ARG24='x25')\n",
    "    pset.renameArguments(ARG25='x26')\n",
    "    pset.renameArguments(ARG26='x27')\n",
    "    pset.renameArguments(ARG27='x28')\n",
    "    pset.renameArguments(ARG28='x29')\n",
    "    pset.renameArguments(ARG29='x30')\n",
    "    pset.renameArguments(ARG30='x31')\n",
    "    pset.renameArguments(ARG31='x32')\n",
    "    pset.renameArguments(ARG32='x33')\n",
    "    pset.renameArguments(ARG33='x34')\n",
    "    pset.renameArguments(ARG34='x35')\n",
    "    pset.renameArguments(ARG35='x36')\n",
    "    pset.renameArguments(ARG36='x37')\n",
    "    pset.renameArguments(ARG37='x38')\n",
    "    pset.renameArguments(ARG38='x39')\n",
    "    pset.renameArguments(ARG39='x40')\n",
    "    pset.renameArguments(ARG40='x41')\n",
    "    pset.renameArguments(ARG41='x42')\n",
    "    pset.renameArguments(ARG42='x43')\n",
    "    pset.renameArguments(ARG43='x44')\n",
    "    pset.renameArguments(ARG44='x45')\n",
    "    pset.renameArguments(ARG45='x46')\n",
    "    pset.renameArguments(ARG46='x47')\n",
    "    pset.renameArguments(ARG47='x48')\n",
    "    pset.renameArguments(ARG48='x49')\n",
    "    pset.renameArguments(ARG49='x50')\n",
    "\n",
    "    # two object types is needed: an individual containing the genotype\n",
    "    # and a fitness -  The reproductive success of a genotype (a measure of quality of a solution)\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n",
    "\n",
    "\n",
    "    #register some parameters specific to the evolution process.\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3) #\n",
    "    toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"compile\", gp.compile, pset=pset)\n",
    "\n",
    "\n",
    "    #evaluation function, which will receive an individual as input, and return the corresponding fitness.\n",
    "    def evalSymbReg(individual):\n",
    "        # Transform the tree expression in a callable function\n",
    "        func = toolbox.compile(expr=individual)\n",
    "        # Evaluate the accuracy of individuals // 1|0 == survived\n",
    "        return math.fsum(np.round(1.-(1./(1.+np.exp(-func(*in_))))) == out for in_, out in zip(inputs, outputs)) / len(evolved_train),\n",
    "\n",
    "\n",
    "    toolbox.register(\"evaluate\", evalSymbReg)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "    toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=3)\n",
    "    toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
    "\n",
    "    toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n",
    "    toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n",
    "\n",
    "    pop = toolbox.population(n=300)\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    #Statistics over the individuals fitness and size\n",
    "    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats_size = tools.Statistics(len)\n",
    "    stats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "\n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.3, ngen=HOWMANYITERS, stats=stats,\n",
    "                                   halloffame=hof, verbose=True)\n",
    "\n",
    "    #Parameters:\n",
    "    #population â A list of individuals.\n",
    "    #toolbox â A Toolbox that contains the evolution operators.\n",
    "    #cxpb â The probability of mating two individuals.\n",
    "    #mutpb â The probability of mutating an individual.\n",
    "    #ngen â The number of generation.\n",
    "    #stats â A Statistics object that is updated inplace, optional.\n",
    "    #halloffame â A HallOfFame object that will contain the best individuals, optional.\n",
    "    #verbose â Whether or not to log the statistics.\n",
    "\n",
    "    # Transform the tree expression of hof[0] in a callable function and return it\n",
    "    func2 = toolbox.compile(expr=hof[0]) \n",
    "\n",
    "    return func2\n",
    "\n",
    "def manualtree(df):\n",
    "    # using manualtree from https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'manual_tree':[]})\n",
    "    male_title = ['Master'] #survived titles\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #Question 1: Were you on the Titanic; majority died\n",
    "        Model.loc[index, 'manual_tree'] = 0\n",
    "\n",
    "        #Question 2: Are you female; majority survived\n",
    "        if (df.loc[index, 'Sex'] == 'female'):\n",
    "                  Model.loc[index, 'manual_tree'] = 1\n",
    "\n",
    "        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n",
    "\n",
    "        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n",
    "        if ((df.loc[index, 'Sex'] == 'female') & \n",
    "            (df.loc[index, 'Pclass'] == 3) & \n",
    "            (df.loc[index, 'Embarked'] == 'S')  &\n",
    "            (df.loc[index, 'Fare'] > 8)\n",
    "\n",
    "           ):\n",
    "                  Model.loc[index, 'manual_tree'] = 0\n",
    "\n",
    "        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n",
    "        if ((df.loc[index, 'Sex'] == 'male') &\n",
    "            (df.loc[index, 'Title'] == 3)\n",
    "            ):\n",
    "            Model.loc[index, 'manual_tree'] = 1\n",
    "        \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "def MungeData(data):\n",
    "\n",
    "    title_list = [\n",
    "                'Dr', 'Mr', 'Master',\n",
    "                'Miss', 'Major', 'Rev',\n",
    "                'Mrs', 'Ms', 'Mlle','Col',\n",
    "                'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer'\n",
    "                                ]\n",
    "\n",
    "    #replacing all people's name by their titles\n",
    "    def replace_names_titles(x):\n",
    "        for title in title_list:\n",
    "            if title in x:\n",
    "                return title\n",
    "    data['Title'] = data.Name.apply(replace_names_titles)\n",
    "    data['Title'] = data['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "    data['Title'] = data['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n",
    "    data['Title'] = data.Title.map({ 'Dr':1, 'Mr':2, 'Master':3, 'Miss':4, 'Major':5, 'Rev':6, 'Mrs':7, 'Ms':8, 'Mlle':9,\n",
    "                     'Col':10, 'Capt':11, 'Mme':12, 'Countess':13, 'Don': 14, 'Jonkheer':15\n",
    "                    })\n",
    "    data = data.drop(['Name'],axis = 1)\n",
    "    data.Title.fillna(0, inplace=True)\n",
    "    data['Is_Married'] = 0\n",
    "    data['Is_Married'].loc[data['Title'] == 7] = 1\n",
    "    # manual_tree\n",
    "    data[\"manual_tree\"] = manualtree(data)\n",
    "    # Age\n",
    "    data['Age'] = data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "    # Relatives\n",
    "    data['Relatives'] = data.SibSp + data.Parch\n",
    "    # Fare per person\n",
    "    data['Fare_per_person'] = data.Fare / np.mean(data.SibSp + data.Parch + 1)\n",
    "    #data.drop(['Fare'], inplace=True, axis=1)\n",
    "    med_fare = data.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "    data = data.drop(['SibSp', 'Parch'], axis=1)\n",
    "    # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n",
    "    data['Fare'] = data['Fare'].fillna(med_fare)\n",
    "    # Ticket\n",
    "    # Sex\n",
    "    data.Sex.fillna('0', inplace=True)\n",
    "    data.loc[data.Sex != 'male', 'Sex'] = 0\n",
    "    data.loc[data.Sex == 'male', 'Sex'] = 1\n",
    "    data['Ticket_Frequency'] = data.groupby('Ticket')['Ticket'].transform('count')\n",
    "    data = data.drop(['Ticket'], axis=1)\n",
    "    # Cabin\n",
    "    data.Cabin.fillna('0', inplace=True)\n",
    "    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1\n",
    "    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 1\n",
    "    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 1\n",
    "    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 2\n",
    "    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 2\n",
    "    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 3\n",
    "    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 3\n",
    "    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 3\n",
    "    # Embarked\n",
    "    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n",
    "    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n",
    "    data.loc[data.Embarked == 'S', 'Embarked'] = 3\n",
    "    data.Embarked.fillna(3, inplace=True)\n",
    "    #data.fillna(0, inplace=True)\n",
    "    #print(data.columns)#data[\"Survived\"] = svd_tmp\n",
    "    data[\"Cabin\"] = data[\"Cabin\"].astype(int)\n",
    "    # now for encoding - first we scale numeric features. E.g. Fare will have bigger values as Age, which \n",
    "    # could confuse an algorithm. therefore we normalize values in the range (-1,1)\n",
    "    numeric_features = ['Relatives','Fare_per_person', 'Fare', 'Age','Ticket_Frequency']\n",
    "    for feature in numeric_features:  \n",
    "        x = data[feature].values #returns a numpy array\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x.reshape(-1, 1) )\n",
    "        data[feature] = pd.DataFrame(x_scaled)\n",
    "\n",
    "    # Categorial features\n",
    "    # Now the best thing for algorithms to work with categories is to have the category values in different\n",
    "    # columns as either 1 or 0. \n",
    "    cat_features = ['Pclass','Embarked', 'Sex', 'Cabin', 'Title','manual_tree','Is_Married']\n",
    "    encoded_features = []\n",
    "    for feature in cat_features:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(data[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = data[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = data.index\n",
    "        encoded_features.append(encoded_df)\n",
    "    data = pd.concat([data, *encoded_features], axis=1)\n",
    "    return data.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# load data for your genetic algorithm\n",
    "raw_train = pd.read_csv('../input/titanic/train.csv')\n",
    "raw_test = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "pass_id_train = raw_train[\"PassengerId\"] # copy it before deleting it in the MungeData step\n",
    "survived_train = raw_train[\"Survived\"] # copy it before deleting it in the MungeData step\n",
    "pass_id_test = raw_test[\"PassengerId\"] # copy it before deleting it in the MungeData step\n",
    "\n",
    "evolved_train = MungeData(raw_train)\n",
    "evolved_test = MungeData(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t      \t                                 fitness                                 \t                      size                     \n",
      "   \t      \t-------------------------------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg     \tgen\tmax     \tmin     \tnevals\tstd      \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t0.597864\t0  \t0.813692\t0.210999\t300   \t0.0804866\t4.55333\t0  \t15 \t2  \t300   \t2.98672\n",
      "1  \t228   \t0.613651\t1  \t0.813692\t0.20651 \t228   \t0.0676165\t5.61333\t1  \t21 \t1  \t228   \t3.79611\n",
      "2  \t237   \t0.622454\t2  \t0.820426\t0.177329\t237   \t0.0762343\t6.52333\t2  \t24 \t1  \t237   \t4.28207\n",
      "3  \t228   \t0.626857\t3  \t0.820426\t0.20651 \t228   \t0.102847 \t7.77333\t3  \t24 \t1  \t228   \t4.68351\n",
      "4  \t217   \t0.650658\t4  \t0.836139\t0.230079\t217   \t0.111272 \t8.96667\t4  \t27 \t1  \t217   \t4.96107\n",
      "5  \t230   \t0.664321\t5  \t0.836139\t0.252525\t230   \t0.126176 \t9.27   \t5  \t30 \t1  \t230   \t5.32702\n",
      "6  \t240   \t0.705357\t6  \t0.836139\t0.186308\t240   \t0.112932 \t9.61667\t6  \t31 \t2  \t240   \t5.54825\n",
      "7  \t230   \t0.71878 \t7  \t0.83165 \t0.20651 \t230   \t0.124597 \t10.31  \t7  \t41 \t1  \t230   \t6.83378\n",
      "8  \t246   \t0.724617\t8  \t0.837262\t0.179574\t246   \t0.134659 \t10.1667\t8  \t38 \t1  \t246   \t6.66775\n",
      "9  \t225   \t0.724504\t9  \t0.837262\t0.195286\t225   \t0.15348  \t10.3467\t9  \t38 \t1  \t225   \t7.01188\n",
      "10 \t225   \t0.752286\t10 \t0.836139\t0.179574\t225   \t0.127486 \t11.3   \t10 \t35 \t1  \t225   \t7.03965\n"
     ]
    }
   ],
   "source": [
    "# Starts the genetic function. Remember this can have a huge comp. effort depending on the value you set \n",
    "# above in HOWMANYITERS\n",
    "GeneticFunctionObject = GP_deap(evolved_train)\n",
    "# optional, save our genetic function for later, good idea if computation took ages\n",
    "with open(\"geneticfunction.pickle\",\"wb\") as file:\n",
    "    pickle.dump(GeneticFunction,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score based on Train set (Remember, Kaggle/Test set score will be different):\n",
      "0.8372615039281706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evolved_train = evolved_train.drop([\"PassengerId\",\"Survived\"],axis=1) # drop PassengerId because we will not use it\n",
    "\n",
    "train_nparray = evolved_train.values.tolist() \n",
    "\n",
    "trainPredictions = Outputs(np.array([GeneticFunctionObject(*x) for x in train_nparray]))\n",
    "print(\"Your score based on Train set (Remember, Kaggle/Test set score will be different):\")\n",
    "print(accuracy_score(survived_train.astype(int),trainPredictions.astype(int)))\n",
    "pd_train = pd.DataFrame({'PassengerId': pass_id_train.astype(int),\n",
    "                        'Predicted': trainPredictions.astype(int),\n",
    "                        'Survived': survived_train.astype(int)})\n",
    "pd_train.to_csv('gptrain_yourgenalgo.csv', index=False)\n",
    "\n",
    "# Test set submission\n",
    "evoled_test = evolved_test.drop([\"PassengerId\"],axis=1) # drop PassengerId because we will not use it\n",
    "test_nparray = evolved_test.values.tolist()\n",
    "testPredictions = Outputs(np.array([GeneticFunctionObject(*x) for x in test_nparray]))\n",
    "\n",
    "pd_test = pd.DataFrame({'PassengerId': pass_id_test.astype(int),\n",
    "                        'Survived': testPredictions.astype(int)})\n",
    "pd_test.to_csv('submission_yourgenalgo.csv', index=False) # change this to \"submission.csv\" only if you want\n",
    "                                                       # to submit results to kaggle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
